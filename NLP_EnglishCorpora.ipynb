{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-EnglishCorpora.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgIKJelvSwF3O1lW7c4pgd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaVardhanGara/Colab/blob/master/NLP_EnglishCorpora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8repu7D6I-OY"
      },
      "source": [
        "                                              **  Tasks**\n",
        "Analysis using existing NLP tools:\n",
        "\n",
        "1.Perform sentence segmentation and word tokenization on the downloaded corpus.\n",
        "For both the tasks, try to explore at least two different methods given by the tools:\n",
        "Using NLTK sentence_tokenizer, word_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfl-ZGO7sYOL",
        "outputId": "0c8ec28a-ed5a-4ac3-a30c-9b0fda6f8ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "#nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.probability import FreqDist\n",
        "f = open('en_wiki.txt')\n",
        "sample = f.read()\n",
        "word_tokens = word_tokenize(sample)\n",
        "sent_tokens = sent_tokenize(sample)\n",
        "print(len(word_tokens))\n",
        "len(sent_tokens)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19602236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "761582"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC1qeBVklt8R"
      },
      "source": [
        "2.Find all possible unigrams,Bigrams and Trigrams calculate their frequencies and plot the frequency distribution\n",
        "\n",
        "Finding number of unigrams, Bigrams and trigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPPo6Hy-lKH4",
        "outputId": "1bbd4edb-439b-4124-9718-f7c9b75b3113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Number of unigrams are:\", len(word_tokens))\n",
        "from nltk.util import bigrams, trigrams, ngrams\n",
        "bigrams=list(nltk.bigrams(word_tokens))\n",
        "print(\"Number of bigrams are:\", len(bigrams))\n",
        "trigrams=list(nltk.trigrams(word_tokens))\n",
        "print(\"Number of trigrams are:\", len(trigrams))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams are: 19602236\n",
            "Number of bigrams are: 19602235\n",
            "Number of trigrams are: 19602234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4j508IQmU0v"
      },
      "source": [
        "Finding Frequencies and frequency distribution of ngrams:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tAEHK_N6Vfr",
        "outputId": "3aad96e4-3b65-43b4-8edb-ba2e8373a1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist=nltk.FreqDist(word_tokens)\n",
        "print(\"Number of distinct unigrams\", len(fdist))\n",
        "#FreqDist.plot(395757)\n",
        "bidist=nltk.FreqDist(bigrams)\n",
        "print(\"Number of distinct bigrams\",len(bidist))\n",
        "#FreqDist.plot(len(bidist))\n",
        "tridist=nltk.FreqDist(trigrams)\n",
        "print(\"Number of distinct trigrams\",len(tridist))\n",
        "#FreqDist.plot(len(tridist))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "395757\n",
            "4095732\n",
            "10827502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39E8jZEmpAzi"
      },
      "source": [
        "Printing 10 most frequent and 10 least frequent words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7GS9ZlvuWQ0",
        "outputId": "b4c5e9c5-bc41-4b51-dc37-603d34068bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "fdist_top10=fdist.most_common(100)\n",
        "print(\"Most frequent are:\", fdist_top10)\n",
        "fdist_least10=fdist.most_common()[-100:-1]\n",
        "print(\"Most frequent are:\", fdist_least10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most frequent are: [('the', 1083392), (',', 1037808), ('.', 757183), ('of', 646178), ('and', 498233), ('in', 377175), ('to', 354649), ('a', 333421), (\"''\", 222661), ('``', 216424), ('was', 210600), ('The', 185104), ('is', 164328), (')', 138703), ('(', 138551), ('as', 135557), ('for', 134955), ('with', 122534), ('by', 111722), ('that', 111073), (\"'s\", 105968), ('were', 102616), ('%', 101860), ('on', 100558), ('from', 94024), ('or', 74671), ('his', 72322), ('at', 70592), ('In', 66696), ('are', 65124), ('an', 63776), ('which', 63495), ('had', 58807), ('it', 53520), ('be', 53497), ('he', 48846), ('has', 40502), ('not', 39793), ('also', 39082), ('have', 35903), ('who', 35636), ('age', 35338), (';', 34681), ('their', 34599), (':', 32740), ('but', 32675), ('first', 31067), ('its', 30889), ('this', 30477), ('one', 29760), ('years', 28550), ('been', 28122), ('other', 27357), ('two', 26105), ('city', 25596), ('more', 25355), ('all', 25250), ('there', 25076), ('population', 24858), ('$', 24754), ('under', 23031), ('It', 22903), ('they', 22733), ('over', 22060), ('He', 21819), ('18', 21808), ('such', 21763), ('into', 21396), ('can', 21276), ('used', 20526), ('after', 20244), ('time', 19953), ('would', 19740), ('A', 19668), ('out', 19038), ('most', 18745), ('This', 18717), ('her', 18508), ('when', 18030), ('only', 17972), ('people', 17799), ('made', 17780), (\"'\", 17750), ('than', 17462), ('There', 17269), ('some', 17249), ('living', 16778), ('United', 16720), ('up', 16608), ('no', 16553), ('between', 16452), ('median', 16016), ('income', 15994), ('many', 15992), ('average', 15979), ('them', 15812), ('including', 15764), ('new', 15157), ('For', 14874), ('American', 14856)]\n",
            "Most frequent are: [('Haha‚ÄìHung', 1), ('êë¶êëô', 1), ('êë¶êë£', 1), ('/-ing/', 1), ('bottom-loop', 1), ('letter-height', 1), ('mistake‚Äîhe', 1), ('/ng/', 1), ('reversed‚Äîintentionally', 1), ('not‚Äîare', 1), ('êë∫', 1), ('êëß', 1), ('êëª', 1), ('êë≥', 1), ('Vandenbrink', 1), ('U+10450‚ÄìU+1047F', 1), ('Multilingual', 1), ('ConScript', 1), ('near-circular', 1), ('discus-shaped', 1), ('ellipticities', 1), ('e=0.7', 1), ('E7', 1), ('M49', 1), ('M59', 1), ('M87', 1), ('4125', 1), ('bar-like', 1), ('tuning-fork', 1), ('Sbc', 1), ('Sb', 1), ('Sc', 1), ('SBb', 1), ('M31', 1), ('M81', 1), ('M104', 1), ('M51a', 1), ('M91', 1), ('M95', 1), ('NGC1672', 1), ('2536', 1), ('2903', 1), ('Face-on', 1), ('dust-lanes', 1), ('Sandage', 1), ('M86', 1), ('5866', 1), (\"'Magellanic\", 1), ('Sm', 1), ('M82', 1), ('1427A', 1), ('8080/85-based', 1), ('single-tasking', 1), ('Z80-specific', 1), ('end-of-word', 1), ('PIP.COM', 1), ('STAT.COM', 1), ('utililty', 1), ('OS/8', 1), ('builtin', 1), ('Peripheral-Interchange-Program', 1), ('model-specific', 1), ('user-installed', 1), ('128-byte', 1), ('byte-exact', 1), ('de-blocking', 1), ('1KB', 1), ('2KB', 1), ('double-density', 1), ('32KB', 1), ('lication', 1), ('bank-switched', 1), ('Debugging', 1), ('bug-killer', 1), ('SmartKey', 1), ('pre-configured', 1), ('COPY', 1), ('initiizing', 1), ('techie', 1), ('Shugart', 1), ('DECsystem-10', 1), ('Program/Monitor', 1), ('PL/P', 1), ('CP/CMS', 1), ('wife/business', 1), ('Intel-contracted', 1), ('Kildalls', 1), ('change-of-name', 1), ('SoftCard', 1), ('most-popular', 1), ('luggables', 1), ('CP/M-capable', 1), ('PCW', 1), ('Multiplan', 1), ('Telengard', 1), ('Gorillas', 1), ('Hamurabi', 1), ('user-written', 1), ('XMODEM', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NGFvNjHGAV6",
        "outputId": "083a1282-6b74-4269-f742-5f9df3ef81c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "labels,values = zip(*fdist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(word_tokens)\n",
        "req_freq = (0.9)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of unigrams required for 90% coverage before stemming of english corpora: \", count)\n",
        "labels,values = zip(*bidist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(bigrams)\n",
        "req_freq = (0.8)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of bigrams required for 80% coverage before stemming of english corpora: \", count)\n",
        "labels,values = zip(*tridist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(trigrams)\n",
        "req_freq = (0.7)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of trigrams required for 70% coverage before stemming of english corpora: \", count)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams required for 90% coverage before stemming of english corpora:  13970\n",
            "Number of bigrams required for 80% coverage before stemming of english corpora:  723390\n",
            "Number of trigrams required for 70% coverage before stemming of english corpora:  4946832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnUtQW0VMLFW"
      },
      "source": [
        "Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0pLP49SMJ10",
        "outputId": "39e81bc4-2e60-49a6-cf0a-f634963a6913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "stem_tokens=[]\n",
        "for word in word_tokens:\n",
        "  word=ps.stem(word)\n",
        "  stem_tokens.append(word)\n",
        "sdist=nltk.FreqDist(stem_tokens)\n",
        "print(\"Number of distinct words after stemming:\" ,len(sdist))\n",
        "stembigrams=list(nltk.bigrams(stem_tokens))\n",
        "stembidist=nltk.FreqDist(stembigrams)\n",
        "stemtrigrams=list(nltk.trigrams(stem_tokens))\n",
        "stemtridist=nltk.FreqDist(stemtrigrams)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of distinct words after stemming: 305198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ublbkAAlWD1N",
        "outputId": "23163bcc-a7b4-407e-fdb5-96139816d2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "labels,values = zip(*sdist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(word_tokens)\n",
        "req_freq = (0.9)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of unigrams required for 90% coverage after stemming of english corpora: \", count)\n",
        "labels,values = zip(*stembidist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(bigrams)\n",
        "req_freq = (0.8)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of bigrams required for 80% coverage after stemming of english corpora: \", count)\n",
        "labels,values = zip(*stemtridist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(trigrams)\n",
        "req_freq = (0.7)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of trigrams required for 70% coverage after stemming of english corpora: \", count)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams required for 90% coverage after stemming of english corpora:  5704\n",
            "Number of bigrams required for 80% coverage after stemming of english corpora:  422392\n",
            "Number of trigrams required for 70% coverage after stemming of english corpora:  4228490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_sKmWRhCVa0"
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi0y8562_tER",
        "outputId": "1717566e-5999-4d8a-d358-5d3d3370730f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stem = PorterStemmer()\n",
        "print(\"lemmatisation on 5 mostt frequent words:\")\n",
        "word = \"between\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"income\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"average\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"including\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"american\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemmatisation on 5 mostt frequent words:\n",
            "Lemmatized Word: between\n",
            "Stemmed Word: between\n",
            "Lemmatized Word: income\n",
            "Stemmed Word: incom\n",
            "Lemmatized Word: average\n",
            "Stemmed Word: averag\n",
            "Lemmatized Word: include\n",
            "Stemmed Word: includ\n",
            "Lemmatized Word: american\n",
            "Stemmed Word: american\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKCJy9TRwnQt",
        "outputId": "0261cf10-54c1-4580-91be-29e26d8fc496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(\"lemmatisation on 5 mostt frequent words:\")\n",
        "word = \"luggables\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"multiplan\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"telengrad\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"hamurabi\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))\n",
        "word = \"user-written\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemmatisation on 5 mostt frequent words:\n",
            "Lemmatized Word: luggables\n",
            "Stemmed Word: luggabl\n",
            "Lemmatized Word: multiplan\n",
            "Stemmed Word: multiplan\n",
            "Lemmatized Word: telengrad\n",
            "Stemmed Word: telengrad\n",
            "Lemmatized Word: hamurabi\n",
            "Stemmed Word: hamurabi\n",
            "Lemmatized Word: user-written\n",
            "Stemmed Word: user-written\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}