{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-EnglishCorpora.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEIaj2AlMCBgIYp+BH9ywO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaVardhanGara/Colab/blob/master/NLP_EnglishCorpora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8repu7D6I-OY"
      },
      "source": [
        "                                              **  Tasks**\n",
        "Analysis using existing NLP tools:\n",
        "\n",
        "1.Perform sentence segmentation and word tokenization on the downloaded corpus.\n",
        "For both the tasks, try to explore at least two different methods given by the tools:\n",
        "Using NLTK sentence_tokenizer, word_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfl-ZGO7sYOL",
        "outputId": "0c8ec28a-ed5a-4ac3-a30c-9b0fda6f8ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "#nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.probability import FreqDist\n",
        "f = open('en_wiki.txt')\n",
        "sample = f.read()\n",
        "word_tokens = word_tokenize(sample)\n",
        "sent_tokens = sent_tokenize(sample)\n",
        "print(len(word_tokens))\n",
        "len(sent_tokens)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19602236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "761582"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC1qeBVklt8R"
      },
      "source": [
        "2.Find all possible unigrams,Bigrams and Trigrams calculate their frequencies and plot the frequency distribution\n",
        "\n",
        "Finding number of unigrams, Bigrams and trigrams\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPPo6Hy-lKH4",
        "outputId": "1bbd4edb-439b-4124-9718-f7c9b75b3113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Number of unigrams are:\", len(word_tokens))\n",
        "from nltk.util import bigrams, trigrams, ngrams\n",
        "bigrams=list(nltk.bigrams(word_tokens))\n",
        "print(\"Number of bigrams are:\", len(bigrams))\n",
        "trigrams=list(nltk.trigrams(word_tokens))\n",
        "print(\"Number of trigrams are:\", len(trigrams))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams are: 19602236\n",
            "Number of bigrams are: 19602235\n",
            "Number of trigrams are: 19602234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4j508IQmU0v"
      },
      "source": [
        "Finding Frequencies and frequency distribution of ngrams:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tAEHK_N6Vfr",
        "outputId": "3aad96e4-3b65-43b4-8edb-ba2e8373a1ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist=nltk.FreqDist(word_tokens)\n",
        "print(\"Number of distinct unigrams\", len(fdist))\n",
        "#FreqDist.plot(395757)\n",
        "bidist=nltk.FreqDist(bigrams)\n",
        "print(\"Number of distinct bigrams\",len(bidist))\n",
        "#FreqDist.plot(len(bidist))\n",
        "tridist=nltk.FreqDist(trigrams)\n",
        "print(\"Number of distinct trigrams\",len(tridist))\n",
        "#FreqDist.plot(len(tridist))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "395757\n",
            "4095732\n",
            "10827502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39E8jZEmpAzi"
      },
      "source": [
        "Printing 10 most frequent and 10 least frequent words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7GS9ZlvuWQ0",
        "outputId": "9a018a88-9697-447e-be09-edc0976dbfe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "fdist_top10=fdist.most_common(10)\n",
        "print(\"Most frequent are:\", fdist_top10)\n",
        "fdist_least10=fdist.most_common()[-10:-1]\n",
        "print(\"Most frequent are:\", fdist_least10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most frequent are: [('the', 1083392), (',', 1037808), ('.', 757183), ('of', 646178), ('and', 498233), ('in', 377175), ('to', 354649), ('a', 333421), (\"''\", 222661), ('``', 216424)]\n",
            "Most frequent are: [('luggables', 1), ('CP/M-capable', 1), ('PCW', 1), ('Multiplan', 1), ('Telengard', 1), ('Gorillas', 1), ('Hamurabi', 1), ('user-written', 1), ('XMODEM', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NGFvNjHGAV6",
        "outputId": "083a1282-6b74-4269-f742-5f9df3ef81c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "labels,values = zip(*fdist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(word_tokens)\n",
        "req_freq = (0.9)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of unigrams required for 90% coverage before stemming of english corpora: \", count)\n",
        "labels,values = zip(*bidist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(bigrams)\n",
        "req_freq = (0.8)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of bigrams required for 80% coverage before stemming of english corpora: \", count)\n",
        "labels,values = zip(*tridist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(trigrams)\n",
        "req_freq = (0.7)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of trigrams required for 70% coverage before stemming of english corpora: \", count)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams required for 90% coverage before stemming of english corpora:  13970\n",
            "Number of bigrams required for 80% coverage before stemming of english corpora:  723390\n",
            "Number of trigrams required for 70% coverage before stemming of english corpora:  4946832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnUtQW0VMLFW"
      },
      "source": [
        "Stemming:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0pLP49SMJ10",
        "outputId": "39e81bc4-2e60-49a6-cf0a-f634963a6913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "stem_tokens=[]\n",
        "for word in word_tokens:\n",
        "  word=ps.stem(word)\n",
        "  stem_tokens.append(word)\n",
        "sdist=nltk.FreqDist(stem_tokens)\n",
        "print(\"Number of distinct words after stemming:\" ,len(sdist))\n",
        "stembigrams=list(nltk.bigrams(stem_tokens))\n",
        "stembidist=nltk.FreqDist(stembigrams)\n",
        "stemtrigrams=list(nltk.trigrams(stem_tokens))\n",
        "stemtridist=nltk.FreqDist(stemtrigrams)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of distinct words after stemming: 305198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ublbkAAlWD1N",
        "outputId": "23163bcc-a7b4-407e-fdb5-96139816d2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "labels,values = zip(*sdist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(word_tokens)\n",
        "req_freq = (0.9)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of unigrams required for 90% coverage after stemming of english corpora: \", count)\n",
        "labels,values = zip(*stembidist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(bigrams)\n",
        "req_freq = (0.8)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of bigrams required for 80% coverage after stemming of english corpora: \", count)\n",
        "labels,values = zip(*stemtridist.items())\n",
        "values = list(values)\n",
        "values.sort(reverse=True)\n",
        "N=len(trigrams)\n",
        "req_freq = (0.7)*N\n",
        "count=0\n",
        "for freq in values:\n",
        "  req_freq-=freq\n",
        "  count+=1\n",
        "  if req_freq<=0:\n",
        "   break\n",
        "print(\"Number of trigrams required for 70% coverage after stemming of english corpora: \", count)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unigrams required for 90% coverage after stemming of english corpora:  5704\n",
            "Number of bigrams required for 80% coverage after stemming of english corpora:  422392\n",
            "Number of trigrams required for 70% coverage after stemming of english corpora:  4228490\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_sKmWRhCVa0"
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi0y8562_tER",
        "outputId": "08726eea-2e60-4903-a011-30f99e008576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stem = PorterStemmer()\n",
        "\n",
        "word = \"flying\"\n",
        "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
        "print(\"Stemmed Word:\",stem.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lemmatized Word: fly\n",
            "Stemmed Word: fli\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}